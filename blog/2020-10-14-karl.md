---
layout: post
title: "KAR$^3$L Progress Update, October 2020"
author: "Shi Feng"
---

<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
	<script src="https://cdn.jsdelivr.net/npm/vega-lite@4"></script>
	<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
</head>

[KAR$^3$L](http://karl.qanta.org/) is a next-generation spaced repetition learning tool developed by the natural language processing lab at University of Maryland ([CLIP lab](https://wiki.umiacs.umd.edu/clip/index.php/Main_Page)). Unlike traditional methods where all flashcards are treated as equal and all users are treated as equal, KAR$^3$L reads the content of each flashcard and the study history of each individual user, then chooses a time to review the flashcard in order to mxaimizes the psychological spacing effect.

In this post, we summarize our first phase tests. By analyzing the study history of our users, we provide some insights into how KAR$^3$L works, its advantages over traditional methods, and where it can be improved. We'll also give a sneak peek of what's coming next.

We are happy to discuss this project. Joining our [Discord](https://discord.com/invite/PTfEmHd), or reach out to our team directly: [@ihsgnef](https://twitter.com/ihsgnef), [@matthewmshu](https://twitter.com/@matthewmshu), and [feet-thinking@googlegroups.com](feet-thinking@googlegroups.com).

## Where we are right now?
Since our [public launch](https://hsquizbowl.org/forums/viewtopic.php?f=123&p=379140&sid=8ae602e914bc1e56736a07030176c718) on August 24th, 424 users signed up, and produced in total over 75,000 study records. Each study record consists of the user ID, flashcard ID, date of study, and whether the user answered correctly. The growth in number of users and records is shown below.

<div id="vis1"></div>

In addition to developing a new tool for studying, we also want to answer the scientific question of whether machine learning can improve spaced repetition. For this purpose, we implemented two traditional models: [Leitner](https://en.wikipedia.org/wiki/Leitner_system) and [SM-2](https://en.wikipedia.org/wiki/SuperMemo), and compare them against our model. Some users who signed up are randomly selected to these traditional methods as control groups. Below is a breakdown of how many users are assigned to each model, and how many records each group contributed.

(plot of accumulative number of studies with breakdown by repetition model, takeaway?)

## Overview of KAR$^3$L vs. Leitner vs. SM-2
Now we take a closer look at how our model performs. First, we want to get a basic understanding of what kind of flashcards are shown to the users. We categorize each study record by both the result (corret or wrong) and whether the flashcard shown is new. This gives us an rough idea of how each model handles the trade-off between showing new flashcards versus reviewing old ones, and how the users respond to them (successful or failed recall). The figure belows visualizes this breakdown of study records in to these $2\times2=4$ categories, and how this breakdown changes as the users spend more time studying. Click on the legend to highlight each category.

<div id="vis2"></div>

We start to see some difference between KAR$^3$L and other models. In particular, KAR$^3$L is showing a higher ratio of new flashcards than Leitner and SM-2, but the ratio of successful reviews is increasing at a slower pace than Leitner. But do these differences make KAR$^3$L better (or worse) at helping users make progress? To answer this question, we first need to come up with some metrics to gauge both the progress and the effort from each user.

## Progress vs. Effort

Our metric for progress is inspired by Ebbinghaus' notion of "first successful reproduction", Leitner's boxes, and the [progress graphs](https://ankiweb.net/shared/info/266436365) of [Anki](https://apps.ankiweb.net/).

Progress is made when a user correctly recalls a flashcard. However, not all correct recalls are equally indicative of the user's progress. For example, the first successful recall of a flashcard is more "valuable" than the recall of a card that the user is already familiar with. We use the notion of __levels__ to quantify how familiar each flashcard is to the user, based on past evaluation: a Level.$X$ flashcard is one that the user has correctly recall $x$ times in a row. We label new flashcards (not seen before) as "Initial" to differentiate them from Level.0 flashcards—old ones whose latest evaluation was unsuccessful. With lines visualizing the cumulative counts of both successful and failed recalls, we can see how the user progresses on each level as days go by. In the figure below, we can also see how the progress is contrasted with the user's effort on each day. Click on the legend to highlight each level.

<div id="vis3"></div>

Perhaps a more informative view of this data is to look at the ratio of successful/total evaluations—recall rate—on each level. This is shown for the same user in the figure below. Click on the legend to highlight each level.

<div id="vis4"></div>

Now, what would this figure above look like if we had the perfect model? Well, we might want the Initial recall rate to be lower, because currently more than 50% of the flashcards shown to this user are already known prior to study. We might want the Level.1-Level.3 recall rates to be lower and closer to 50%, because according to Ebbinghaus each repetition boost the memory strength by 50%, so to maximize the gain each review should happen when the recall rate is about 50%. Currently the recall rate is at around 80% for higher level cards, which means we are probably reviewing those cards too soon; the effort spent on those cards could have been better used for less familiar flashcards.

To compare KAR$^3$L against Leitner and SM-2 in a similar visualization, we aggregate users that are assigned to each scheduler, and make x-axis represent number of minutes the user spent on the app. In the two figures below the band visualizes the standard deviation of the corresponding line. Click on the legend to highlight each level.

<div id="vis5"></div>
<div id="vis6"></div>

The second figure shows some interesting differences between KAR$^3$ and traditional methods. First, zooming in on Initial flashcards, we see that the recall rate is higher in KAR$^3$ is higher than the other two models, and shows lower variance both among users and over time. This is partly because KAR$^3$L explicitly controls for the difficulty and topic of new flashcards, as opposed to randomly selecting them as done in the other two models. The recall rate of Initial cards in KAR$^3$L might be a bit too high, but it's something we can control; we'll dig more in a second.

Zooming in on the Level.0 cards, again we see lower variance in recall rate from KAR$^3$L, but the mean is on similar to Leitner and SM-2. However, if we look at Level.1 cards, the recall rate from KAR$^3$L users is noticeably higher than the other two models, although there is a slight dip in the most recent data.

The analysis above seems to suggest that the recall rate in KAR$^3$L is in general a bit higher than ideal. Luckily, KAR$^3$L is designed with this kind of flexibility in mind. One of the tunable parameters of KAR$^3$L is _recall target_, which specifies the desired recall rate for a card to be reviewed. For example, if the recall target is set to 80%, the model prioritizes flashcards whose probability of recall by the user—according to the model's prediction—is closest to 80%. So recall target is one of the most important factors that together controls what flahscards are shown to the user.[^1] The default recall target was set to 100% (a bad idea in hindsight), which partially explains why the recall rate is so high.

After receiving some user feedback that KAR$^3$L is reviewing too much, which corroborates our findings here, we created a new experimental condition with recall target set to a lower 85%, and assigned some new users to this condition. The figure below compares the recall rate of cards on each leve between the two versions of KAR$^3$L. This change is quite recent so there are fewer users in this group (thus higher variance), and the users haven't spent as much time on the app yet (thus shorter lines).

[^1]: We don't go into the details of how our model works in this report. But we hope to release a document specifically for that purpose soon. Stay tuned!

<div id="vis7"></div>

This comparison sheds some insight into how the recall target parameter affects the model behavior. As we lower the recall target from $100%$ to 85%, we see that the Initial and Level.0 recall rates become lower, as expected. However, there is some noticeable mismatch between the recall target and the actual recall rate, and for Initial and Level.0 flashcards, the actual recall rate is lower. Weirdly, the recall rate at Level.1 and Level.2 did not drop as significantly, but we can't draw much of a conclusion due to limited data. We hypothesize that the inconsistency of model behavior with respect to recall target is caused by two issues: how our model adapts to each user, and the overconfidence of the neural network.[^2] Although this issue might require some smart technical solution, we see this as a positive signal: KAR$^3$L has the potential to be much more flexible than traditional methods, and now we have data to fine-tune its paramaters. The difference in learning curves also highlights the room for further optimizing learning efficiency via machine learning. This brings us to our next steps. 

[^2]: Neural models are known to be over-confident in their predictions without calibration. See [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) for a reference.

## Next Steps
We identify three main tasks: in depth analysis of KAR$^3$L's behavior, learn from cognitive science and educational theory literature to improve & expand evaluation, and improve the feedback loop between the users and the models. We'll briefly explain what we want to achieve in each of the tasks, and highlight what's coming soon next.

### In depth analysis of KAR$^3$L's behavior
We want to understand the inconsistency of KAR$^3$L's behavior with respect to different recall targets, and come up with a remedy. Another important task is to find a good recall target that balances efficiency & fun. We plan to look into educational theory for inspiration. We'll test the model in simulation using data we have collected so far, and come up with new experimental groups for a better user experience.

### Better evaluation
Our current evaluation, especially the definition of level, is closely related to Anki's notion of "learned" & "mature", as well as the boxes in Leitner system. Our progress graph is good for drawing insights, but not rigorous for a comparison between models—it's not standardized. We would want to test the users with either the same set of flashcards, or flashcards of the same objective difficulty.

### Feedback loop between user and model
We want to provide more feedback to the users, sharing insights on their progress, similar to what we did in this report. In the update that is released with this report, the progress graphs are added to the Statistics page for each user.

We also want to allow the users to provide feedback more directly. In the next phase of experiment, we plan to give the users more control over KAR$^3$L's settings, such as the recall target and topical preference.

## Summary

<script type="text/javascript">
  vegaEmbed('#vis1', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/n_users_and_n_records.json").catch(console.error);
  vegaEmbed('#vis2', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/new_old_correct_wrong.json").catch(console.error);
  vegaEmbed('#vis3', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/463_user_level_vs_effort.json").catch(console.error);
  vegaEmbed('#vis4', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/463_user_level_ratio.json").catch(console.error);
  vegaEmbed('#vis5', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/repetition_model_level_vs_effort.json").catch(console.error);
  vegaEmbed('#vis6', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/repetition_model_level_ratio.json").catch(console.error);
  vegaEmbed('#vis7', "https://raw.githubusercontent.com/ihsgnef/ihsgnef.github.io/master/images/100vs85_level_ratio.json").catch(console.error);
</script>
